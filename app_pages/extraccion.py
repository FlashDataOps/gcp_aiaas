import streamlit as st
import pandas as pd
import numpy as np
import re
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferWindowMemory
from dotenv import load_dotenv

load_dotenv()

left_co, cent_co,last_co = st.columns([1,5,1])
with cent_co:
    image_url = "images/botimagen.png"
    st.image(image_url, use_column_width=True)

# T√≠tulo y subt√≠tulo centrados
st.markdown("""
    <div style='text-align: center;'>
        <h1>üëì Extractor de campos</h1>
        <h3>Sube un archivo de cliente del que extraer los campos</h3>
    </div>
    """, unsafe_allow_html=True)

# Configuraci√≥n en la barra lateral
with st.sidebar:
    st.title("Men√∫ de configuraci√≥n")

    files = st.file_uploader("Suba aqu√≠ sus documentos")
    ramos = ['Autom√≥viles', 'Vida']
    dropdown = st.selectbox("Elija el ramo que quiere investigar", ramos)
    
    if files is not None:
        # Leer el contenido del archivo
        files = files.read().decode("utf-8")

# Inicializar historial de chat y memoria
if "messages" not in st.session_state:
    st.session_state.messages = []

if "dfs" not in st.session_state:  # Diccionario para guardar los DataFrames
    st.session_state.dfs = {}

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferWindowMemory(k=5)

# for message in st.session_state.messages:
#     avatar = 'ü§ñ' if message["role"] == "assistant" else 'üë®‚Äçüíª'
#     with st.chat_message(message["role"], avatar=avatar):
#         st.markdown(message["content"])
#         if message["role"] == "assistant":
#             if st.session_state.df is not st.session_state.df.empty:
#                 st.dataframe(st.session_state.df, hide_index = True, use_container_width=True)

for i, message in enumerate(st.session_state.messages):
    avatar = 'ü§ñ' if message["role"] == "assistant" else 'üë®‚Äçüíª'
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])
        if message["role"] == "assistant" and i in st.session_state.dfs:
            df = st.session_state.dfs[i]
            if df is not df.empty:  
                st.dataframe(df, hide_index=True, use_container_width=True)

# Procesar la pregunta del usuario
if question := st.chat_input("Enter your prompt here..."):
    st.session_state.messages.append({"role": "user", "content": question})

    prompt = ChatPromptTemplate.from_messages([
        ('system', """
        Eres un asistente especialista en la extracci√≥n de campos relevantes de archivos.
        Debes comenzar la respuesta con un p√°rrafo introductorio en el que destaques alg√∫n punto de informaci√≥n clave del cliente como su nombre o el seguro que quieren contratar.
        Debes extraer siempre todos los campos que haya en el archivo.
        No muestres los campos en el p√°rrafo inicial.
        La informaci√≥n extra√≠da solo debe mostrarse en la tabla del c√≥digo de python.
        Debes dar una respuesta continua, evita hacer secciones.
        Antes de la tabla di: Esta es la informaci√≥n extra√≠da del cliente:
        Siempre tienes que crear un c√≥digo python que pueda ser ejecutado directamente.
        Vas a recibir informaci√≥n de un cliente y vas a extraer campos de informaci√≥n relevante.
        Despu√©s del p√°rrafo inicial, debes generar un c√≥digo en Python para mostrar los campos extra√≠dos. El c√≥digo de Python debe
        crear un √∫nico dataframe de Pandas usando Streamlit y lo muestre en un st.dataframe().
        La tabla debe tener un formato de dos columnas y tantas filas como campos haya.
        En st.dataframe() mete el par√°metro: hide_index=True
        En st.dataframe() mete el par√°metro: use_container_width=True
        
        Informaci√≥n que vas a utilizar para responder: {files}
        """),
        ("placeholder", "{chat_history}"),
        ('user', "{question}")
    ])

    # Usar el modelo para generar la respuesta
    llm = ChatGroq(model="llama3-70b-8192")
    chain = (prompt | llm | StrOutputParser())

    with st.chat_message("user", avatar='üë®‚Äçüíª'):
        st.markdown(question)

        # Obtener la respuesta
        response = chain.invoke({"question": question, "files":files, "chat_history": st.session_state.memory.chat_memory.messages})

    # Mostrar la respuesta y procesarla en una tabla
    with st.chat_message("assistant", avatar="ü§ñ"):
    # Usamos una expresi√≥n regular para extraer el bloque de c√≥digo Python
        text_response = response.split("```")[0].strip()
        text_response
        
        try:
            # Primer intento: buscar cualquier bloque entre triple backticks (``` ... ```)
            code_match = re.search(r"```(.*?)```", response, re.DOTALL)

        # Verificar si el bloque encontrado comienza con "python"
            if code_match.group(1).strip().startswith("python"):
                # Eliminar "python" al inicio si est√° presente
                python_code = code_match.group(1).strip()[len("python"):].strip()
                
                local_scope = {}
                exec(python_code, {}, local_scope)
            else:
                # Si no inicia con "python", simplemente usar el contenido extra√≠do
                python_code = code_match.group(1).strip()

                local_scope = {}
                exec(python_code, {}, local_scope)

        except Exception as e:
            st.error(f"Hubo un error al procesar los datos: {e}")
    

    # if "df" in local_scope:
    #  st.session_state.df = local_scope["df"]
    
    if "df" in local_scope:
        df = local_scope["df"]
        st.session_state.dfs[len(st.session_state.messages)] = df  # Asociar al mensaje actual
    else:
        df = None

    # Agregar la tabla en el historial de chat
    st.session_state.messages.append({
        "role": "assistant",
        "content": text_response
    })

    st.session_state.memory.save_context(
        {"Human": question},  # Inputs
        {"AI": text_response}  # Output
    )